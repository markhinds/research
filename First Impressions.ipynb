{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, set, component, transform):\n",
    "        self.root_dir = root_dir\n",
    "        self.set = set\n",
    "        self.component = component\n",
    "        self.transform = transform\n",
    "\n",
    "        self.total_imgs, self.total_values = self.get_data_list()\n",
    "        self.total_values = np.array(self.total_values).astype(np.float)\n",
    "        self.gt_mean = np.mean(self.total_values)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = self.total_imgs[idx]\n",
    "        try:\n",
    "            image = Image.open(open(img_loc, 'rb')).convert(\"RGB\")\n",
    "        except:\n",
    "            print(img_loc, 'was not loaded')\n",
    "\n",
    "        # Normalize the image data\n",
    "        image = self.transform(image)\n",
    "        image = torch.tensor((image.numpy() - np.mean(image.numpy())) / np.std(image.numpy()))\n",
    "\n",
    "        value = self.total_values[idx]\n",
    "\n",
    "        return image, value\n",
    "\n",
    "    def get_data_list(self):\n",
    "        image_list, value_list = [], []\n",
    "        img_path = self.root_dir + \"Data\\\\Image\\\\\"\n",
    "        anno_path = self.root_dir + \"Data\\\\Annotation\\\\\" + self.set + self.component\n",
    "\n",
    "        print('Anno file: ', anno_path)\n",
    "\n",
    "        f = open(anno_path,'r')\n",
    "        for i,line in enumerate(f):\n",
    "            splitted_line = line.split(',')\n",
    "            image_list.append(img_path+splitted_line[0])\n",
    "            value_list.append(splitted_line[1])\n",
    "        f.close()\n",
    "\n",
    "        print(len(image_list), len(value_list))\n",
    "\n",
    "        return image_list, value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataLoader():\n",
    "    def __init__(self, datadir, set, component, batch_size, n_workers, shuffle):\n",
    "        self.datadir = datadir\n",
    "        self.set = set\n",
    "        self.component = component\n",
    "        self.batch_size = batch_size\n",
    "        self.n_workers = n_workers\n",
    "        self.shuffle = shuffle\n",
    "        self.trans = transforms.Compose([\n",
    "            transforms.Pad((10, 0, 10, 0)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def get_data_loader(self):\n",
    "        dataset = FaceDataset(self.datadir, self.set, self.component, self.trans)\n",
    "        data_loader = data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "            num_workers=self.n_workers,\n",
    "            drop_last=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        print('\\ndata len:', dataset.__len__(), '\\n')\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import models, transforms\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "class CNNmodel(pl.LightningModule):\n",
    "    def __init__(self, batch_size=6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.predictions, self.ground_truths = [],[]\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            *list(models.vgg16(pretrained=True).children())[:-2],\n",
    "        )\n",
    "\n",
    "        self.valueDecoder = torch.nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Fully connected layer\n",
    "            nn.Linear(512*4*4, 4608),\n",
    "            nn.Linear(4608, 2048),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(32, 1),\n",
    "            # output: 1\n",
    "        )\n",
    "\n",
    "        self.faceDecoder = torch.nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 3, kernel_size=3, stride=2),\n",
    "            nn.ConvTranspose2d(3, 3, kernel_size=3, stride=2),\n",
    "            nn.ConvTranspose2d(3, 3, kernel_size=3, stride=2),\n",
    "            nn.ConvTranspose2d(3, 3, kernel_size=3, stride=2,),\n",
    "            # output = 3 x 95 x 95\n",
    "        )\n",
    "\n",
    "        # self.weights_init(self.encoder)\n",
    "        # self.weights_init(self.hm3d_xy_Decoder)\n",
    "        # self.weights_init(self.hm3d_yz_Decoder)\n",
    "        # self.weights_init(self.hm3d_xz_Decoder)\n",
    "\n",
    "    def init_layer(self, layer, classname):\n",
    "        if classname.find('Conv') != -1:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "        elif classname.find('Linear') != -1:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            layer.weight.data.fill_(1)\n",
    "            layer.bias.data.zero_()\n",
    "        elif classname.find('Bottle') != -1:\n",
    "            nn.init.xavier_uniform_(layer.conv1.weight)\n",
    "            nn.init.xavier_uniform_(layer.conv2.weight)\n",
    "            nn.init.xavier_uniform_(layer.conv3.weight)\n",
    "\n",
    "    def weights_init(self, model, is_resnet=False):\n",
    "        if is_resnet:\n",
    "            for layer in model:\n",
    "                classname = layer.__class__.__name__\n",
    "                if 'Seq' in classname:\n",
    "                    for subLayer in layer:\n",
    "                        subclassname = subLayer.__class__.__name__\n",
    "                        self.init_layer(subLayer, subclassname)\n",
    "                else:\n",
    "                    self.init_layer(layer, classname)\n",
    "        else:\n",
    "            for layer in model:\n",
    "                classname = layer.__class__.__name__\n",
    "                self.init_layer(layer, classname)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print('img size:', x.size())\n",
    "        x_enout = self.encoder(x)\n",
    "        x_valout = self.valueDecoder(x_enout)\n",
    "        x_faceout = self.faceDecoder(x_enout)\n",
    "        # print('x: ', x.size(), ', x_valout: ', x_valout.size(), ', x_faceout: ', x_faceout.size()) \n",
    "\n",
    "        return x, x_valout, x_faceout\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image, value = batch\n",
    "        tensor_value = torch.zeros(self.batch_size, 1).cuda()\n",
    "        for i in range(self.batch_size):\n",
    "            tensor_value[i][0] = value[i]\n",
    "        x, x_val, x_face = self.forward(image)\n",
    "\n",
    "        lambda_value = 10\n",
    "        lambda_face = 0.001\n",
    "\n",
    "        # print('x_val:', x_val.size(), 'tensor_value:', tensor_value.size())\n",
    "        loss_value = self.cust_loss(x_val, tensor_value) * lambda_value\n",
    "\n",
    "        new_gt = self.resize_gt(image.cuda(), x_face.size())\n",
    "        loss_face = F.mse_loss(x_face, new_gt, reduction='sum') * lambda_face\n",
    "\n",
    "        loss = loss_value + loss_face\n",
    "\n",
    "        self.log_dict({\n",
    "            'L': loss,\n",
    "            'L_v': loss_value,\n",
    "            'L_face': loss_face,\n",
    "        }, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        image, value = batch\n",
    "        tensor_value = torch.zeros(self.batch_size, 1).cuda()\n",
    "        for i in range(self.batch_size):\n",
    "            tensor_value[i] = value[i]\n",
    "        x, x_val, x_face = self.forward(image)\n",
    "\n",
    "        val_loss = F.mse_loss(x_val, tensor_value, reduction='sum') * 10\n",
    "\n",
    "        self.log('val_loss', val_loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        image, value = batch\n",
    "        tensor_value = torch.zeros(self.batch_size, 1).cuda()\n",
    "        for i in range(self.batch_size):\n",
    "            tensor_value[i][0] = value[i]\n",
    "        x, x_val, x_face = self.forward(image)\n",
    "\n",
    "        x_val = x_val.cpu().detach().numpy()\n",
    "        tensor_value = tensor_value.cpu().detach().numpy()\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            self.ground_truths.append(tensor_value[i])\n",
    "            self.predictions.append(x_val[i])\n",
    "        \n",
    "        loss_r2 = r2_score(x_val, tensor_value)\n",
    "\n",
    "        self.log_dict({\n",
    "            'R2': loss_r2,\n",
    "        }, prog_bar=True)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-6)\n",
    "        return optimizer\n",
    "\n",
    "    def cust_loss(self, pred, gt):\n",
    "        loss = 0\n",
    "        for i in range(self.batch_size):\n",
    "            tmp = gt[i][0] - pred[i][0]\n",
    "            if tmp > 0:\n",
    "                loss += tmp**2\n",
    "            else:\n",
    "                loss += (tmp * 2) ** 2\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def r2_loss(self, pred, gt, gt_mean):\n",
    "        # m = torch.ones(gt.size()) * gt_mean\n",
    "        ss_tot = torch.sum((gt - gt_mean) ** 2)\n",
    "        ss_res = torch.sum((gt - pred) ** 2)\n",
    "        r2 = 1 - ss_res / ss_tot\n",
    "        return r2\n",
    "    \n",
    "    def resize_gt(self, gt, pred_size):\n",
    "        new_gt = torch.zeros(pred_size).cuda()\n",
    "        trans = transforms.Compose([\n",
    "            transforms.Resize(pred_size[-1]),\n",
    "        ])\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            new_gt[i] = trans(gt[i])\n",
    "\n",
    "        return new_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:\\Users\\hinds\\CSResearch\\\"\n",
    "set = 'validate'\n",
    "component = 'Age' \n",
    "batch_size = 64\n",
    "n_workers = 0\n",
    "shuffle = True\n",
    "train_loader = FaceDataLoader(data_dir,set,component,batch_size,n_workers,shuffle).get_data_loader()\n",
    "train_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplace import Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, data in enumerate(train_loader):\n",
    "    print('batch {}, shape {}'.format(batch_idx, data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# root = 'C:\\\\Users\\\\hinds\\\\CSResearch\\\\Data\\\\Image'\n",
    "# my_csv_file = ...\n",
    "\n",
    "# # Loading csv as {image:class,...} format\n",
    "# df = pd.read_csv(my_csv_file).set_index('images')\n",
    "# class_dict = df.idxmax(axis=\"columns\").to_dict()\n",
    "\n",
    "# # Moving files to class-named subfolders\n",
    "# for path in Path(root).iterdir():\n",
    "#     if path.is_file() and path.name in class_dict.keys():\n",
    "#         path.rename(Path(path.parent, class_dict[path.name], path.name)\n",
    "\n",
    "# # Loading dataset\n",
    "# dataset = torchvision.datasets.ImageFolder(root=root, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = torch.nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "for X,y in train_loader:\n",
    "    loss = crit(model(X), y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "#evaluating already trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.fit(train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
